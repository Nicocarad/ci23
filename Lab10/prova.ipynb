{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3))\n",
    "        self.players = [\"X\", \"O\"]\n",
    "        self.current_player = None\n",
    "        self.winner = None\n",
    "        self.game_over = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3))\n",
    "        self.current_player = None\n",
    "        self.winner = None\n",
    "        self.game_over = False\n",
    "\n",
    "    def available_moves(self):\n",
    "        moves = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if self.board[i][j] == 0:\n",
    "                    moves.append((i, j))\n",
    "        return moves\n",
    "\n",
    "    def make_move(self, move):\n",
    "        if self.board[move[0]][move[1]] != 0:\n",
    "            return False\n",
    "        self.board[move[0]][move[1]] = self.players.index(self.current_player) + 1\n",
    "        self.check_winner()\n",
    "        self.switch_player()\n",
    "        return True\n",
    "\n",
    "    def switch_player(self):\n",
    "        if self.current_player == self.players[0]:\n",
    "            self.current_player = self.players[1]\n",
    "        else:\n",
    "            self.current_player = self.players[0]\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Check rows\n",
    "        for i in range(3):\n",
    "            if self.board[i][0] == self.board[i][1] == self.board[i][2] != 0:\n",
    "                self.winner = self.players[int(self.board[i][0] - 1)]\n",
    "                self.game_over = True\n",
    "        # Check columns\n",
    "        for j in range(3):\n",
    "            if self.board[0][j] == self.board[1][j] == self.board[2][j] != 0:\n",
    "                self.winner = self.players[int(self.board[0][j] - 1)]\n",
    "                self.game_over = True\n",
    "        # Check diagonals\n",
    "        if self.board[0][0] == self.board[1][1] == self.board[2][2] != 0:\n",
    "            self.winner = self.players[int(self.board[0][0] - 1)]\n",
    "            self.game_over = True\n",
    "        if self.board[0][2] == self.board[1][1] == self.board[2][0] != 0:\n",
    "            self.winner = self.players[int(self.board[0][2] - 1)]\n",
    "            self.game_over = True\n",
    "\n",
    "    def print_board(self):\n",
    "        print(\"-------------\")\n",
    "        for i in range(3):\n",
    "            print(\"|\", end='' '')\n",
    "            for j in range(3):\n",
    "                print(self.players[int(self.board[i][j] - 1)] if self.board[i][j] != 0 else \" \", end='' | '')\n",
    "            print()\n",
    "            print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, epsilon, discount_factor):\n",
    "        self.Q = {}\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def get_Q_value(self, state, action):\n",
    "        if (state, action) not in self.Q:\n",
    "            self.Q[(state, action)] = 0.0\n",
    "        return self.Q[(state, action)]\n",
    "\n",
    "    def choose_action(self, state, available_moves):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(available_moves)\n",
    "        else:\n",
    "            Q_values = [self.get_Q_value(state, action) for action in available_moves]\n",
    "            max_Q = max(Q_values)\n",
    "            if Q_values.count(max_Q) > 1:\n",
    "                best_moves = [i for i in range(len(available_moves)) if Q_values[i] == max_Q]\n",
    "                i = random.choice(best_moves)\n",
    "            else:\n",
    "                i = Q_values.index(max_Q)\n",
    "            return available_moves[i]\n",
    "\n",
    "    def update_Q_value(self, state, action, reward, next_state):\n",
    "        next_Q_values = [self.get_Q_value(next_state, next_action) for next_action in TicTacToe(next_state).available_moves()]\n",
    "        max_next_Q = max(next_Q_values) if next_Q_values else 0.0\n",
    "        self.Q[(state, action)] += self.alpha * (reward + self.discount_factor * max_next_Q - self.Q[(state, action)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_episodes, alpha, epsilon, discount_factor):\n",
    "    agent = QLearningAgent(alpha, epsilon, discount_factor)\n",
    "    for i in range(num_episodes):\n",
    "        state = TicTacToe().board\n",
    "        while not TicTacToe(state).game_over():\n",
    "            available_moves = TicTacToe(state).available_moves()\n",
    "            action = agent.choose_action(state, available_moves)\n",
    "            next_state, reward = TicTacToe(state).make_move(action)\n",
    "            agent.update_Q_value(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent, num_games):\n",
    "    num_wins = 0\n",
    "    for i in range(num_games):\n",
    "        state = TicTacToe().board\n",
    "        while not TicTacToe(state).game_over():\n",
    "            if TicTacToe(state).player == 1:\n",
    "                action = agent.choose_action(state, TicTacToe(state).available_moves())\n",
    "            else:\n",
    "                action = random.choice(TicTacToe(state).available_moves())\n",
    "            state, reward = TicTacToe(state).make_move(action)\n",
    "        if reward == 1:\n",
    "            num_wins += 1\n",
    "    return num_wins / num_games * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TicTacToe.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the Q-learning agent\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Test the Q-learning agent\u001b[39;00m\n\u001b[0;32m      5\u001b[0m win_percentage \u001b[38;5;241m=\u001b[39m test(agent, num_games\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(num_episodes, alpha, epsilon, discount_factor)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m      4\u001b[0m     state \u001b[38;5;241m=\u001b[39m TicTacToe()\u001b[38;5;241m.\u001b[39mboard\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mTicTacToe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgame_over():\n\u001b[0;32m      6\u001b[0m         available_moves \u001b[38;5;241m=\u001b[39m TicTacToe(state)\u001b[38;5;241m.\u001b[39mavailable_moves()\n\u001b[0;32m      7\u001b[0m         action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(state, available_moves)\n",
      "\u001b[1;31mTypeError\u001b[0m: TicTacToe.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# Train the Q-learning agent\n",
    "agent = train(num_episodes=100000, alpha=0.5, epsilon=0.1, discount_factor=1.0)\n",
    "\n",
    "# Test the Q-learning agent\n",
    "win_percentage = test(agent, num_games=1000)\n",
    "print(\"Win percentage: {:.2f}%\".format(win_percentage))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
